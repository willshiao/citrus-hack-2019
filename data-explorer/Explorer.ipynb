{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import spacy\n",
    "import multiprocessing \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load('en_core_web_lg', disable=['parser', 'ner', 'textcat'])\n",
    "nlp = spacy.load('en_vectors_web_lg', disable=['parser', 'tagger', 'ner', 'textcat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFile = \"../data/cleaned.csv\"\n",
    "data1 = pd.read_csv(dataFile)\n",
    "data2 = pd.read_csv('../data/hate_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data2.drop(data2[data2['Class'] == 'neither'].index)\n",
    "data2['Class'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['Class'] == 1].tail(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_whitelist = set(['ADJ', 'ADV', 'PROPN', 'VERB', 'NOUN', 'INTJ', 'PRON', 'X', 'NUM'])\n",
    "whitelist = set(string.ascii_lowercase + string.digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "megaSpam = pd.read_csv('../data/polluters.tsv', sep='\\t')\n",
    "megaNonSpam = pd.read_csv('../data/legit.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "megaSpam['Class'] = 1\n",
    "megaNonSpam['Class'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/will/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>FavCount</th>\n",
       "      <th>RtCount</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Text</th>\n",
       "      <th>TweetID</th>\n",
       "      <th>Verified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Tactical_Things</td>\n",
       "      <td>RT @HouseOfTraitors: FARAGE  12  -  0 Mandelsc...</td>\n",
       "      <td>732588910078001152</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>kataisa</td>\n",
       "      <td>@TheBriefing2016 @HillaryClinton Can Hillary s...</td>\n",
       "      <td>732588910073778176</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5423.0</td>\n",
       "      <td>Josh_Ward_</td>\n",
       "      <td>RT @HQUESTED: Harvey price just said cunt on n...</td>\n",
       "      <td>732588910061232128</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>networkMCI</td>\n",
       "      <td>@xonighttimexo I will put them in a Safety dep...</td>\n",
       "      <td>732588910056902656</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>nbharper</td>\n",
       "      <td>you should listen, but you probably won't bc y...</td>\n",
       "      <td>732588910069620736</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class  FavCount  RtCount       ScreenName  \\\n",
       "0      0       0.0      8.0  Tactical_Things   \n",
       "1      0       0.0      0.0          kataisa   \n",
       "2      0       0.0   5423.0       Josh_Ward_   \n",
       "3      0       0.0      0.0       networkMCI   \n",
       "4      0       0.0      0.0         nbharper   \n",
       "\n",
       "                                                Text             TweetID  \\\n",
       "0  RT @HouseOfTraitors: FARAGE  12  -  0 Mandelsc...  732588910078001152   \n",
       "1  @TheBriefing2016 @HillaryClinton Can Hillary s...  732588910073778176   \n",
       "2  RT @HQUESTED: Harvey price just said cunt on n...  732588910061232128   \n",
       "3  @xonighttimexo I will put them in a Safety dep...  732588910056902656   \n",
       "4  you should listen, but you probably won't bc y...  732588910069620736   \n",
       "\n",
       "   Verified  \n",
       "0       1.0  \n",
       "1       1.0  \n",
       "2       1.0  \n",
       "3       1.0  \n",
       "4       1.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat([data1, data2, megaSpam, megaNonSpam])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Text'].shape[0] // 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found error: nan, <class 'float'>\n",
      "Found error: nan, <class 'float'>\n",
      "Found error: nan, <class 'float'>\n",
      "Found error: nan, <class 'float'>\n",
      "Found error: nan, <class 'float'>\n",
      "Found error: nan, <class 'float'>\n",
      "Found error: nan, <class 'float'>\n",
      "Found error: nan, <class 'float'>\n",
      "CPU times: user 29.9 s, sys: 16.7 s, total: 46.6 s\n",
      "Wall time: 2min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "print(len(X_test), len(y_test))\n",
    "print(len(X_train), len(y_train))\n",
    "\n",
    "# classifier = GradientBoostingClassifier()\n",
    "classifier = XGBClassifier\n",
    "# classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_hat = classifier.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanData = [' '.join([x.lower() for x in y.split(' ') if len(x) > 0 and x[0] != '@']) for y in cleanData]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5638816, 300)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(cleanData)\n",
    "y = data['Class']\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1409704 1409704\n",
      "4229112 4229112\n",
      "CPU times: user 1d 5min 2s, sys: 1min 58s, total: 1d 7min\n",
      "Wall time: 1h 36min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "print(len(X_test), len(y_test))\n",
    "print(len(X_train), len(y_train))\n",
    "\n",
    "# classifier = GradientBoostingClassifier()\n",
    "# classifier = XGBClassifier(n_jobs=30)\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(100, 200, 100), activation='logistic', early_stopping=True)\n",
    "# classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_hat = classifier.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('thepickle_NN.pkl', 'bw') as f:\n",
    "#     pickle.dump([encoder, vectorizer, classifier], f)\n",
    "    pickle.dump(classifier, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanData[:100]\n",
    "for d in cleanData:\n",
    "    if 'null' in d:\n",
    "        print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = CountVectorizer()\n",
    "# for i in range(0, len(cleanData) - 9999, 10000):\n",
    "# #     X = vectorizer.fit_transform(cleanData[i:i+10000])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X.shape\n",
    "# X.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = LabelEncoder()\n",
    "# y = encoder.fit_transform(data[\"Class\"])\n",
    "y = data['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.shape)\n",
    "print(len(cleanData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train, clean_test, y_train, y_test = train_test_split(cleanData, y, test_size=0.25, random_state=42)\n",
    "print(len(clean_test), len(y_test))\n",
    "print(len(clean_train), len(y_train))\n",
    "vectorizer = CountVectorizer()\n",
    "X_vec = vectorizer.fit_transform(clean_train)\n",
    "\n",
    "# classifier = GradientBoostingClassifier()\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_vec, y_train)\n",
    "\n",
    "y_vec = vectorizer.transform(clean_test)\n",
    "y_hat = classifier.predict(y_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = GradientBoostingClassifier()\n",
    "classifier.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 3300334, 1: 2338482})\n",
      "0.5852884719061591 0.414711528093841\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "vals = Counter(y)\n",
    "print(vals)\n",
    "print(vals[0] / len(y), vals[1] / len(y))\n",
    "# Counter(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8149249771583255"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_hat)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_hat)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PICKLE EVERYTHING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('thepickle_LR_wordvec.pkl', 'bw') as f:\n",
    "#     pickle.dump([encoder, vectorizer, classifier], f)\n",
    "    pickle.dump(classifier, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = classifier.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 100\n",
    "top_k = np.argpartition(coefs, -k)[-k:]\n",
    "top_k_sorted=top_k[np.argsort(coefs[top_k])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = vectorizer.get_feature_names()\n",
    "[feats[x] for x in top_k_sorted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
